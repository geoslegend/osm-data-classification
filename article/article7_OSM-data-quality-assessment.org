#+TITLE: OSM data quality assessment: producing map to illustrate data quality
#+AUTHOR: Damien Garaud <damien.garaud@oslandia.com>, RaphaÃ«l Delhome <raphael.delhome@oslandia.com>

# Common introduction for articles of the OSM-data-quality series
At [[http://oslandia.com/][Oslandia]], we like working with Open Source tool projects and handling Open
(geospatial) Data. In this article series, we will play with the [[https://www.openstreetmap.org/][OpenStreetMap]]
(/OSM/) map and subsequent data. Here comes the eighth article of this series,
dedicated to the OSM data quality evaluation, through production of new maps.

* Description of OSM element

** Element metadata extraction

As mentionned in a previous article dedicated to [[http://oslandia.com/en/2017/07/24/osm-metadata-description-the-data-behind-the-data/][metadata extraction]], we have
to focus on element metadata itself if we want to produce valuable information
about quality. The first questions to answer here are straightforward: /what is
an OSM element?/ and /how to extract its associated metadata?/. This part is
relatively similar to the job already done with users.

We know from previous analysis that an element is created during a change set
by a given contributor, may be modified several times by whoever, and may be
deleted as well. This kind of object may be either a "node", a "way" or a
"relation". We also know that there may be a set of different tags associated
with the element. Of course the list of every operations associated to each
element is recorded in the OSM data history, so we have to begin with this
structure (let's consider data around Bordeaux, as in previous blog posts):

#+BEGIN_SRC ipython :session osm
import pandas as pd
elements = pd.read_table('../src/data/output-extracts/bordeaux-metropole/bordeaux-metropole-elements.csv', parse_dates=['ts'], index_col=0, sep=",")
#+END_SRC

#+RESULTS:

This short description helps us to identify some basic features, which are
built in the following snippets. First we recover the temporal features:

#+BEGIN_SRC ipython :session osm
elem_md = (elements.groupby(['elem', 'id'])['ts']
            .agg(["min", "max"])
            .reset_index())
elem_md.columns = ['elem', 'id', 'first_at', 'last_at']
elem_md['lifespan'] = (elem_md.last_at - elem_md.first_at)/pd.Timedelta('1D')
extraction_date = elements.ts.max()
elem_md['n_days_since_creation'] = ((extraction_date - elem_md.first_at)
                                  / pd.Timedelta('1d'))
elem_md['n_days_of_activity'] = (elements
                              .groupby(['elem', 'id'])['ts']
                              .nunique()
                              .reset_index())['ts']
elem_md = elem_md.sort_values(by=['first_at'])
#+END_SRC

#+RESULTS:

Then the remainder of the variables, /e.g./ how many versions, contributors,
changesets per elements:

#+BEGIN_SRC ipython :session osm
    elem_md['version'] = (elements.groupby(['elem','id'])['version']
                          .max()
                          .reset_index())['version']
    elem_md['n_chgset'] = (elements.groupby(['elem', 'id'])['chgset']
                           .nunique()
                           .reset_index())['chgset']
    elem_md['n_user'] = (elements.groupby(['elem', 'id'])['uid']
                         .nunique()
                         .reset_index())['uid']
    elem_md = pd.merge(elem_md, elements[['elem', 'id', 'version', 'visible']],
                       on=['elem', 'id', 'version'])
    elem_md = elem_md.set_index(['elem', 'id'])
    elem_md.sample().T
#+END_SRC

#+RESULTS:
#+begin_example
elem                                  node
id                               278128672
first_at               2008-07-13 00:00:00
last_at                2008-07-13 00:00:00
lifespan                                 0
n_days_since_creation                 3143
n_days_of_activity                       1
version                                  1
n_chgset                                 1
n_user                                   1
visible                               True
#+end_example

As an illustration we have above an old one-versionned node, still visible on
the OSM website.

** Characterize OSM elements with user classification

This set of features is only descriptive, we have to add more information to be
able to characterize OSM data quality. That is the moment to exploit the user
classification produced in the last blog post!

As a recall, we hypothesized that clustering the users permits to evaluate
their trustworthiness as OSM contributors. They are either beginners, or
intermediate users, or even OSM experts, according to previous classification.

Each OSM entity may have received one or more contributions by users of each
group. Let's say the entity quality is good if its last contributor is
experienced. That leads us to classify the OSM entities themselves in return!

/How to include this information into element metadata?/

That's the crucial question, and let's give an answer in this snippet:

#+BEGIN_SRC ipython :session osm

#+END_SRC

** Recover the geometry information

Even if =Pyosmium= library is able to retrieve OSM element geometries, we
realized some test with an other OSM data parser here: =osm2pgsql=

As the main operations, we must:
- build an OSM element metadata table from OSM data history
- parse a geometry-oriented extraction of OSM data with =osm2pgsql=
- merge both tables

* Keep it visual, man!

From the last conclusions, we are able to produce some customized maps, based
on hypothesis on entity quality. If each OSM entities (*e.g.* roads) can be
characterized in terms of quality, by the way of a synthetic score for example,
then we can draw quality maps by highlighting the most trustworthy entities, as
well as those with which we have to stay cautious.

** First step: simple metadata plotting

Number of user per roads, number of tags per roads, and so on, within PostGIS

** More complex: OSM data merging with alternative geospatial representations

Use the INSEE carroyed data to provide an alternative way of OSM data
representation: 200-meter squared tiles, within which OSM element statistics
are aggregated

** The cherry on the cake: representation of OSM elements with respect to quality

Design similar map than in previous section, with user classification
information: /e.g./ last contributor group

Idea: identify questionable OSM elements, that should be verified (they are not
necessarily of low quality!)
