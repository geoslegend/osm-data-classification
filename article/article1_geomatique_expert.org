#+TITLE: A data-oriented framework to assess OSM data quality (part 1): data extraction and description
#+AUTHOR: Damien Garaud <damien.garaud@oslandia.com>, RaphaÃ«l Delhome <raphael.delhome@oslandia.com>, Hugo Mercier <hugo.mercier@oslandia.com>

* Introduction

At [[http://oslandia.com/][Oslandia]], we like working with Open Source tool
projects and handling Open (geospatial) Data. In this article, we will play
with [[https://www.openstreetmap.org/][OpenStreetMap]] (/OSM/) and the
subsequent data.

Here comes a first article dedicated to the presentation of our working
framework, as well as to the presentation of some basic OSM data features
through the chronological evolution of OSM API.

As the reader should know, [[https://www.openstreetmap.org][OpenStreetMap]] is
a project which creates and distributes free geographical data for the
world. Like Wikipedia, it's a community of people who can create and update
some content available for everyone. Thus, anyone can edit buildings, roads,
places or even trees and mailboxes!

Working with community-built data forces to take care of data quality. We have
to be confident with the data we work with. Is this road geometry accurate
enough? Is this street name missing? This is fundamental for companies and NGO
who use OSM on a daily basis.

One curcial purpose is to answer to this question: /can you assess the quality
of OSM data? (and how?)/ Before giving elements to solve this point, a bunch of
methodological aspects and a first overview of OSM data must be proposed. It is
the purpose of this paper.

In this first article, we will present what we mean by /data quality/, in the
context of Geospatial systems. Then we will continue by some methodology
elements: the Python framework useful to exploit OSM data will be
introduced. After that a first OSM data set will be described to illustrate the
parsing process. In a last section the chronological evolution of OSM data will
be assessed to complete the OSM data description.

* State of the art
** Geospatial data quality components

Van Oort (2006) defines several spatial data quality criteria:

- lineage
- positional accuracy
- attribute accuracy
- logical accuracy
- completeness
- semantic accuracy
- usage, purpose, constraints
- temporal quality
- variation in quality
- meta-quality
- resolution

This classification has been recalled in further contributions, however most
studies focus on positional accuracy. For instance, Haklay (2010), Koukoletsos
/et al./ (2011) or Helbich /et al./ (2012) compared OSM data with Ordnance
Survey data, an alternative data source considered as a ground truth.

There are two differences with our approach: we don't have any geospatial data
reference to cope with the positional accuracy. Moreover the authors decided to
take a snapshot of the OSM data instead of using OSM history data.

** OSM contributors and data quality

OSM is a community, as Wikipedia, where everyone can create, edit and delete
entities. You can suppose that the quality of a contribution depends on the
user who made it.

Let's begin by an example: if the user is experienced, you can suppose that the
contribution should be good. And when you don't have reference data to measure
the data accuracy, you can suppose that if the road was created a few years ago
with 20 updates, it should be complete and accurate enough.

Other references follow this point of view. Arsanjani /et al./ (2013)
classified OSM contributors based on the quality and quantity of their
contributions in Heidelberg (Germany). Five classes are used: "beginner",
"regular", "intermediate", "expert", and "professional mappers". The authors
work with reference data in addition to OSM data, they can assess the
positional accuracy of a contribution. Moreover, they take into account the
completness and the semantic accuracy. Then Neis /et al./ (2014) proposed a
whole set of statistics dedicated to OSM contributors. They provide hand-made
groups, and characterize contributions regarding dates, hours, user
localisation and activity.

Additional references can be mentionned to overcome the OSM data qualiy
issue. The [[https://www.iso.org/committee/54904.html][ISO/TC 211 working group]] published a set of norms for geographical
information standardization. For instance, the norm ISO19157:2013 (2013) cites
some of quality attributes mentionned above. See also the Wikipedia notice
about the [[http://wiki.openstreetmap.org/wiki/Quality_assurance][OSM quality assurance]] which lists several tools to supervise the OSM
data construction.

** What sort of data are behind the OpenStreetMap API?

Within the OSM API, a set of seminal entities can be easily identified:

- nodes, characterized by geographical coordinates;
- ways, characterized by a list of nodes;
- relations, characterized by a set of "members", /i.e./ nodes, ways
  or other relations.

In addition to these three element types, a fundamental object is the change
set. It describes a set of modifications done by a single user, during a
limited amount of time.

Each of these OSM objects are characterized by a set of common attributes, that
are IDs, timestamps, visible flags /(is the object still visible on the API?)/,
user IDs, or lists of tags (a tag being the association between a key and a
value).

Starting from these OSM elements, we can straightforwardly answer typical
questions as:

+ How many nodes do each user create?
+ How frequent are the mofification for each contributor?
+ How many tags do each OSM element contain?
+ ...

Considering the history of OSM data makes the data set even more complete: it
allows us to study the temporal evolution of the API.

* A Python framework to exploit OSM Data

Our analysis is mainly based on Python capability. It must be recalled here
that this programming language permits to extract OSM data, to analyze them
conveniently as well as to organize all the workflow, through its different
packages.

** From the OSM history dumps to usable data sets

Extracting OSM data is:

+ simple because you just have to download the history dump in /.pbf/ ([[https://developers.google.com/protocol-buffers/][Protocol
  Buffer]]) or /.osh/ formats from [[https://planet.openstreetmap.org/][Planet OSM website]] (/.osm/ format refers to
  latest data, whereas /.osh/ refers to history data).
+ complex because when you want to extract alternative data, it can be a long
  and tedious task.

The challenge here is to pass from these native formats to in-base data or
/.csv/ files. Several tools exist to accomplish this effort, we detail two of
them here.

When working with /.pbf/ files, we propose here to use the =pyosmium=, a
dedicated [[http://docs.osmcode.org/pyosmium/v2.11.0/][Python library]]. These files are freely available on [[http://download.geofabrik.de/][Geofabrik]] (/e.g./
a [[http://download.geofabrik.de/europe.html][continent]], a [[http://download.geofabrik.de/europe/france.html][country]], or even a [[http://download.geofabrik.de/europe/france/aquitaine.html][sub-region]]) in /osm/ or /osh/ version (/i.e./
up-to-date API or history). For the whole planet, the /.pbf/ file is quite big:
~57Go. Note that the /.xml/ file is compressed with =bzip2=. It can be long
(+36 hours) and take some place (1TB) if you uncompress it.

If we want to extract another free area, [[http://osmcode.org/osmium-tool/][osmium-tool]] is a serious option in
order to create our own /.pbf/ file. It is available as a package in the Debian
GNU/Linux distribution. The next section show an example of utilization, to get
OSM data around Bordeaux, France.

** Python is your friend, Luigi your plumber

In addition to =pyosmium= and to classic data-oriented Python packages (/e.g./
=numpy= and =pandas=), we use [[https://luigi.readthedocs.io/en/stable/][Luigi]], which is dedicated to job pipeline
building. This tool allows to manage every tasks and organize them all
together, by clarifying the dependencies.

A quick benchmark about existing Luigi utilizations shows that machine learning
applications are extremely compatible with this package. That is particularly
true if we consider Map/Reduce frameworks. Here we will demonstrate that Luigi
keeps its interest in our case, with a slightly different usage.

Here our workflow can be illustrated by the following figure, obtained with the help
of the Luigi daemon. It permits to explore the task pipeline graphically as
well as to explore their accomplishment degree while running.

#+CAPTION: Example of Luigi dependency graph
#+NAME: fig:luigi-dep-graph
[[./../figs/luigi_dependency_graph_example.png]]

We can identify some example of tasks in this graph:

- =OSMHistoryParsing= and =OSMTagParsing= are sources, these parsing tasks
  provide initial data sets by using =pyosmium= capacities.
- =OSMElementEnrichment= is an intermediary task in which additional features
  are merged to OSM history data
- These additional features are used in every metadata building tasks:
  =OSMTagMetaAnalysis=, =ElementMetadataExtract=, =ChangeSetMetadataExtract=
  and =UserMetadataExtract=.
- =MasterTask= is an abstract task that yields each final tasks. Its completion
  equals to the success of the pipelined procedure.

This example describes a part of our global framework for illustration purpose;
however this paper will only focus on parsing tasks. Additionnally, other tasks
could of course be put in the pipeline (*e.g.* machine-learning procedures to
classify users); they can be integrated in the framework with a minimal
effort.

* OSM data extraction

** Build our own OSM data sample

We saw in the last section which tool to use if we want to work with specific
OSM area. Here we provide an insight of the methodology with Bordeaux, a
medium-sized French city.

This alternative method needs the area geographical coordinates. We recover
them by drawing the accurate bounding box within the OpenStreetMap [[https://www.openstreetmap.org/#map=10/45.0000/0.0000][API]] export
tool. We get the following bounding box coordinates: the top-left corner is at
={44.9335, -0.7179}= whilst the bottom-right corner is at ={44.7216,
-0.4134}=. These coordinates seem quite weird (weirdly concise!), however they
are just hand-made, by successive zooms in the OSM API.

#+CAPTION: Hand-made bounding box on Bordeaux city (France)
#+NAME: fig:osm-bb-example
#+attr_html: :width 800px
[[./../figs/osm_boundingbox_example.png]]

They are integrated in the following JSON configuration file, as well as the
output file name:

#+BEGIN_SRC js
{ "extracts": [ { "output": "bordeaux-metropole.osh.pbf", "output_format":
  "osh.pbf", "description": "extract OSM history for Bordeaux (France)",
  "bbox": {"left": -0.7179, "right": -0.4134, "top": 44.9335, "bottom":
  44.7216} } ], "directory": "/path/to/outputdir/" }
#+END_SRC

This JSON file is used by osmium to build a standard /pbf/ file in the
following shell command:

#+BEGIN_SRC shell
osmium extract --with-history --config=region.json latest-planet.osh.pbf
#+END_SRC

Where =latest-planet.osh.pbf= is the input file (downloaded from Geofabrik
website, we still need some original data!). The =--with-history= flag here is
important as well. We want to study the temporal evolution of some OSM
entities, the number of contributions, and check some specific OSM entities
such as nodes, ways or relations and get their history.

** Extract OSM data history

At this point, we have a /pbf/ file that contains every OSM element versions
through time. We still have to write them into a /csv/ file. Here we use
[[http://docs.osmcode.org/pyosmium/latest/index.html][pyosmium]] (see previous
article).

This operation can be done through a simple Python file (see snippets below).

#+BEGIN_SRC ipython :session osm :exports both
  import osmium as osm import pandas as pd

  class TimelineHandler(osm.SimpleHandler): 
      def __init__(self):
          osm.SimpleHandler.__init__(self)
          self.elemtimeline = []

      def node(self, n): 
          self.elemtimeline.append(["node", n.id, n.version,n.visible, 
                                    pd.Timestamp(n.timestamp), n.uid, 
                                    n.changeset, len(n.tags)])
#+END_SRC

#+RESULTS:

First we have to import the useful libraries, that are pandas (to handle
dataframes and /csv/ files) and pyosmium. Then, we define a small OSM data
handler, that saves every nodes into the =elemtimeline= attribute (/i.e./ a
list). This example is limited to nodes for a sake of concision, however this
class is easily extensible to other OSM objects. We can observe that several
node attributes are recorded: the element type ("node" for nodes, of course!),
ID, version in the history, if it is currently visible on the API, timestamp
(when the version has been set), user ID, change set ID and the number of
associated tags. These attributes are also available for ways and relations,
letting the chance to put a little more abstraction in this class definition!

An instance of this class can be created so as to save OSM nodes within the
Bordeaux metropole area (see below). We pass the input file name to the
=apply_file= procedure, that scans the input file and fills the handler list
accordingly. After that we just have to transform the list into a pandas
DataFrame, to make further treatments easier.

#+BEGIN_SRC ipython :session osm :exports both
  tlhandler = TimelineHandler()
  tlhandler.apply_file("../src/data/raw/bordeaux-metropole.osh.pbf") colnames =
  ['type', 'id', 'version', 'visible', 'ts', 'uid', 'chgset', 'ntags'] elements
  = pd.DataFrame(tlhandler.elemtimeline, columns=colnames) elements =
  elements.sort_values(by=['type', 'id', 'ts']) elements.head(10)
#+END_SRC

#+RESULTS:
#+begin_example
   type id version visible ts uid chgset \ 0 node 21457126 2 False 2008-01-17
16:40:56+00:00 24281 653744 1 node 21457126 3 False 2008-01-17 16:40:56+00:00
24281 653744 2 node 21457126 4 False 2008-01-17 16:40:56+00:00 24281 653744 3
node 21457126 5 False 2008-01-17 16:40:57+00:00 24281 653744 4 node 21457126 6
False 2008-01-17 16:40:57+00:00 24281 653744 5 node 21457126 7 True 2008-01-17
16:40:57+00:00 24281 653744 6 node 21457126 8 False 2008-01-17 16:41:28+00:00
24281 653744 7 node 21457126 9 False 2008-01-17 16:41:28+00:00 24281 653744 8
node 21457126 10 False 2008-01-17 16:41:49+00:00 24281 653744 9 node 21457126
11 False 2008-01-17 16:41:49+00:00 24281 653744

   ntags 0 0 1 0 2 0 3 0 4 0 5 1 6 0 7 0 8 0 9 0
#+end_example

With the help of pandas library, to save the file into /csv/ format is
straightforward:

#+BEGIN_SRC ipython :session osm :exports both
  elements.to_csv("bordeaux-metropole.csv", date_format='%Y-%m-%d %H:%M:%S')
#+END_SRC

At this point, the OSM data history is available in a /.csv/ file format, coming
with a whole set of attributes that will be useful to describe the data.

* OSM API exploration through time

** A simple procedure to build dated OSM histories

From the OSM data history we can recover the current state of OSM data (or more
precisely, the API state at the data extraction date). The only step that is
needed is to select the up-to-date OSM objects, /i.e./ those with the last
existing version, through a =group-by= operation.

#+BEGIN_SRC ipython :session osm :exports both
  def updatedelem(data): updata =
      data.groupby(['type','id'])['version'].max().reset_index() return
      pd.merge(updata, data, on=['id','version']) uptodate_elem =
      updatedelem(elements) uptodate_elem.head()
#+END_SRC

This seem to be a quite useless function: we could have found directly such
data on GeoFabrik website, isn't it? ... Well, it is not that useless. As an
extension of this first procedure, we propose a simple but seminal procedure
called =datedelems= that allows us to get the OSM API picture given a specific
date:

#+BEGIN_SRC ipython :session osm :exports both
  def datedelems(history, date): datedelems = (history.query("ts <= @date")
      .groupby(['type','id'])['version'] .max() .reset_index()) return
      pd.merge(datedelems, history, on=['type','id','version'])

  oldelem = datedelems(elements, "2008-02-01") oldelem.head()
#+END_SRC

#+RESULTS:
#+begin_example
   type id version visible ts uid chgset \ 0 node 21457126 48 False 2008-01-17
16:42:01+00:00 24281 653744 1 node 21457144 9 False 2008-01-17 16:45:43+00:00
24281 653744 2 node 21457152 6 True 2008-01-17 16:45:39+00:00 24281 653744 3
node 21457164 5 False 2008-01-17 16:48:00+00:00 24281 653744 4 node 21457175 4
False 2008-01-17 16:47:51+00:00 24281 653744

   ntags 0 0 1 0 2 1 3 0 4 0
#+end_example

We can notice in this function that pandas allows to express queries in a
SQL-like mode, a very useful practice in order to explore data!

As a corollary we can build some time series aiming to describe the evolution
of the API in terms of OSM objects (nodes, ways, relations) or users.

** How to get the OSM API evolution?

What if we consider OSM API state month after month? What is the temporal
evolution of node, way, or relation amounts? The following procedure helps us
to describe the OSM API at a given date: how many node/way/relation there are,
how many user have contributed, how many change sets have been opened. Further
statistics may be designed, in the same manner.

#+BEGIN_SRC ipython :session osm :exports both
  def osm_stats(osm_history, timestamp): osmdata = datedelems(osm_history,
      timestamp) nb_nodes = len(osmdata.query('type == "node"')) nb_ways =
      len(osmdata.query('type == "way"')) nb_relations =
      len(osmdata.query('type == "relation"')) nb_users = osmdata.uid.nunique()
      nb_chgsets = osmdata.chgset.nunique() return [nb_nodes, nb_ways,
      nb_relations, nb_users, nb_chgsets]

  osm_stats(elements, "2014-01-01")
#+END_SRC

#+RESULTS:
| 2166480 | 0 | 0 | 528 | 9345 |

Here we do not get any way or relation, that seems weird, doesn't it? However,
do not forget how the parser was configured above ! By tuning it so as to
consider these OSM element types, this result is modified.

By designing a last function, we can obtain a pandas dataframe that summarizes
basic statistics at regular timestamps: in this example, we focus on monthly
evaluations, however everything is possible... A finner analysis is possible,
by taking advantage of pandas time series capabilities.

#+BEGIN_SRC ipython :session osm :exports both
  def osm_chronology(history, start_date, end_date): timerange =
      pd.date_range(start_date, end_date, freq="1M").values osmstats =
      [osm_stats(history, str(date)) for date in timerange] osmstats =
      pd.DataFrame(osmstats, index=timerange, columns=['n_nodes', 'n_ways',
      'n_relations', 'n_users', 'n_chgsets']) return osmstats
#+END_SRC

#+RESULTS:

These developments open further possibilities. Areas are comparable through
their history. A basic hypothesis could be: some areas have been built faster
than others, /e.g./ urban areas /vs/ desert areas. To investigate on the
evolutions of their OSM objects appears as a very appealing way to address this
issue!

** What about the Bordeaux area?

To illustrate the previous points, we can call the =osm_chronology= procedure
to Bordeaux-related OSM data. We can study the last 10 years, as an example:

#+BEGIN_SRC ipython :session osm :exports both
  chrono_data = osm_chronology(elements, "2007-01-01", "2017-01-01")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session osm :exports both
  pd.concat([chrono_data.iloc[:10,[0,3,4]], chrono_data.iloc[-10:,[0,3,4]]])
#+END_SRC

#+RESULTS:
#+begin_example
            n_nodes n_users n_chgsets 2007-01-31 24 1 2 2007-02-28 24 1 2
2007-03-31 45 3 4 2007-04-30 45 3 4 2007-05-31 1744 4 8 2007-06-30 1744 4 8
2007-07-31 1744 4 8 2007-08-31 3181 6 12 2007-09-30 3186 7 15 2007-10-31 3757 8
18 2016-03-31 2315763 882 15280 2016-04-30 2318044 900 15468 2016-05-31 2321910
918 15841 2016-06-30 2325689 931 16153 2016-07-31 2329592 942 16613 2016-08-31
2334206 955 16835 2016-09-30 2337157 973 17005 2016-10-31 2339526 1004 17462
2016-11-30 2342109 1014 17637 2016-12-31 2349670 1028 17933
#+end_example

The figure below describes the evolution of nodes, ways and relations around
Bordeaux between 2007 and 2017, as well as the number of users and change
sets. The graphes are log-scaled, for a sake of clarity.

We can see that the major part of Bordeaux cartography has been undertaken
between fall of 2010 and spring of 2013, with a clear peak at the beginning
of 2012. This evolution is highly pronounced for nodes or even ways, whilst the
change set amount and the contributor quantity increased regularly. This may
denote the differences in terms of user behaviors: some of them create only a
few objects, while some others contributes with a large amount of created
entities.

#+CAPTION: Amount of OSM objects in the area of Bordeaux (France)
#+NAME: fig:bm-chronology
#+attr_html: :width 800px
[[./../figs/bordeaux-metropole-chronology-logscale.png]]

As a remark, the number of active contributor plotted here is not really
representative of the total of OSM contributors: we consider only local data
here. Active users all around the world are not those who have collaborated for
this specific region. However the change set and user statistics for
full-planet dumps exist, if you are interested in going deeper about this
point!

** Opening case study: comparing several french areas

Before concluding this article, here is provided a comparison between OSM node
amounts in several french areas. We just mention small areas, to keep the
evaluation short: Upper Normandy, a roughly rural environment with some
medium-sized cities (Rouen, Le Havre, Evreux...), Corsica, an montainous island
near to mainland France and French Guiana, an overseas area mainly composed of
jungle. The figure below shows the difference between these areas in terms of
OSM nodes and active contributors. To keep the comparison as faithful as
possible, we have divided these amounts by each surface area: respectively
12137, 8680 and 83534 square kilometers for Upper Normandy, Corsica and French
Guiana.

#+CAPTION: Amount of OSM nodes in several french areas
#+NAME: fig:multiarea-chronology-nodes
#+attr_html: :width 800px
[[./../figs/multiarea-chronology-weighted.png]]

Without any surprise, it is the mainland area (Upper Normandy) that is the most
dense on OSM. This area contains almost 700 nodes per square kilometer (quite
modest, however we talk about a rural area!). We can notice that they are
almost the same number of contributors between Normandy and Corsica. On the
other hand, French Guiana is an extrem example, as expected! There are less
than 15 nodes and 0.01 contributor per square kilometer. We have identified a
OSM desert, [[https://www.openstreetmap.org/#map=8/4.072/-52.844 ][welcome to
the Guiana jungle]] ! (You can act on it: be environment-friendly,
[[http://wiki.openstreetmap.org/wiki/How_to_contribute][plant some more
trees]]!)


==================


* Conclusion


* References

- Arsanjani, J, Barron, C, Bakillah, M, Helbich, M. 2013. Assessing
  the quality of OpenStreetMap contributors together with their
  contributions. /Proceedings of the AGILE./ p14-17.
- Haklay, M. 2010. How good is volunteered geographical information? A
  comparative study of OpenStreetMap and Ordnance Survey datasets. /Environment
  and planning B: Planning and design./ 37(4), p.682-703.
- Helbich, M, Amelunxen, C, Neis, P, Zipf, A. 2012. Comparative
  spatial analysis of positional accuracy of OpenStreetMap and proprietary
  geodata. /Proceedings of GI Forum./ p.24-33.
- ISO. 2013. Geographic information: data
  quality. /ISO19157:2013./ Geneva, Switzerland: ISO.
- Koukoletsos, T, Haklay, M, Ellul, C. 2011. An automated method to
  assess data completeness and positional accuracy of
  OpenStreetMap. /GeoComputation./ 3, p.236-241.
- Neis, P, Zipf, A. 2012. Analyzing the contributor activity of a
  volunteered geographic information project: the case of OpenStreetMap. /ISPRS
  International Journal of Geo-Information, Molecular Diversity Preservation./
  1, p.146-165.
- Van Oort, P. 2006. Spatial data quality: from description to
  application. /PhD report./ Wageningen Universiteit.
