#+TITLE: A data-oriented framework to assess OSM data quality (part 1): data extraction and description
#+AUTHOR: Damien Garaud <damien.garaud@oslandia.com>, RaphaÃ«l Delhome <raphael.delhome@oslandia.com>, Hugo Mercier <hugo.mercier@oslandia.com>

* Introduction

At Oslandia, we like working with Open Source tool projects and handling Open
(geospatial) Data. In this article, we will play with [[https://www.openstreetmap.org/][OpenStreetMap]] (/OSM/) and
the subsequent data.

Here comes a first article dedicated to the presentation of our working
framework, as well as to the presentation of some basic OSM data features
through the chronological evolution of the API.

As the reader should know, OSM is a project which creates and distributes free
geographical data for the world. Like Wikipedia, it's a community of people who
can create and update some content available for everyone. Thus, anyone can
edit buildings, roads, places or even trees and mailboxes!

Working with community-built data forces to take care of data quality. We have
to be confident with the data we work with. Is this road geometry accurate
enough? Is this street name missing? This is fundamental for companies and NGO
who use OSM on a daily basis.

One crucial purpose is to answer this question: /can you assess the quality of
OSM data? (and how?)/ Before giving elements to solve this point, a bunch of
methodological aspects and a first overview of OSM data must be proposed. It is
the purpose of this paper.

In this first article, we will present what we mean by /data quality/, in the
context of Geospatial systems. Then we will continue by some methodology
elements: the Python framework useful to exploit OSM data will be
introduced. After that a first OSM data set will be described to illustrate the
parsing process. In a last section the chronological evolution of OSM data will
be assessed to complete the OSM data description.

* State of the art
** Geospatial data quality components

Van Oort (2006) defines several spatial data quality criteria:

- lineage
- positional accuracy
- attribute accuracy
- logical accuracy
- completeness
- semantic accuracy
- usage, purpose, constraints
- temporal quality
- variation in quality
- meta-quality
- resolution

This classification has been recalled in further contributions, however most
studies focus on positional accuracy. For instance, Haklay (2010), Koukoletsos
/et al./ (2011) or Helbich /et al./ (2012) compared OSM data with Ordnance
Survey data, an alternative data source considered as a ground truth.

There are two differences with our approach: we don't have any geospatial data
reference to cope with the positional accuracy. Moreover the authors decided to
take a snapshot of the OSM data instead of using OSM history data.

** OSM contributors and data quality

OSM is a community where everyone can create, edit and delete entities. We can
suppose that the quality of a contribution depends on the user who made it.

Let us begin by an example: if the user is experienced, the contribution should
be good. And when we do not have reference data to measure the data accuracy,
we can suppose that if the road was created a few years ago with many updates,
it should be complete and accurate enough.

Other references follow this point of view. Arsanjani /et al./ (2013)
classified OSM contributors based on the quality and quantity of their
contributions in Heidelberg (Germany). Five classes are used: "beginner",
"regular", "intermediate", "expert", and "professional mappers". The authors
work with reference data in addition to OSM data, they can assess the
positional accuracy of a contribution. Moreover, they take into account the
completness and the semantic accuracy. Then Neis /et al./ (2014) proposed a
whole set of statistics dedicated to OSM contributors. They provide hand-made
groups, and characterize contributions regarding dates, hours, user
localisation and activity.

Additional references can be mentionned to overcome the OSM data qualiy
issue. The ISO/TC 211 working group published a set of norms for geographical
information standardization. For instance, the norm ISO19157:2013 (2013) cites
some of quality attributes mentionned above. See also the Wikipedia notice
about the [[http://wiki.openstreetmap.org/wiki/Quality_assurance][OSM quality assurance]] which lists several tools to supervise the OSM
data construction.

** What sort of data are behind the OpenStreetMap API?

Within the OSM API, a set of seminal entities can be easily identified:

- nodes, characterized by geographical coordinates;
- ways, characterized by a list of nodes;
- relations, characterized by a set of "members", /i.e./ nodes, ways
  or other relations.

In addition to these three element types, a fundamental object is the change
set. It describes a set of modifications done by a single user, during a
limited amount of time.

Each of these OSM objects are characterized by a set of common attributes, that
are IDs, timestamps, visible flags /(is the object still visible on the API?)/,
user IDs, or lists of tags (a tag being the association between a key and a
value).

Starting from these OSM elements, we can straightforwardly answer typical
questions:

+ How many nodes does each user create?
+ How frequent are the mofifications for each contributor?
+ How many tags does each OSM element contain?
+ ...

Considering the history of OSM data makes the data set even more complete: it
allows us to study the temporal evolution of the API.

* A Python framework to exploit OSM Data

Our analysis is mainly based on Python capability. It must be recalled here
that this programming language permits to extract OSM data, to analyze them
conveniently as well as to organize all the workflow, through its different
packages.

** From the OSM history dumps to usable data sets

Extracting OSM data is:

+ simple because you just have to download the history dump in /.pbf/ (Protocol
  Buffer) format, based on /.xml/. The corresponding files have two possible extensions: /.osm/
  refers to up-to-date data, whereas /.osh/ refers to history data.
+ complex because when you want to extract alternative data, it can be a long
  and tedious task.

The challenge here is to pass from these native formats to in-base data or
/.csv/ files. Several tools exist to accomplish this effort, two of them are
detailed here.

When working with /.pbf/ files, =pyosmium=, a dedicated Python library, is a
really useful tool. These files are freely available on [[http://download.geofabrik.de/][Geofabrik]] (/e.g./ a
continent, a country, or even a sub-region) in /osm/ or /osh/ versions. The
whole planet dump is the biggest file, its size reaches around 60Go.

If we want to extract another free area, =osmium-tool= is a serious option in
order to create our own /.pbf/ file. It is available as a package in the Debian
GNU/Linux distribution. The next section show an example of utilization, to get
OSM data around Bordeaux, France.

** Python is your friend, Luigi your plumber

In addition to =pyosmium= and to classic data-oriented Python packages (/e.g./
=numpy= and =pandas=), we use =Luigi=, which is dedicated to job pipeline
building. This tool allows to manage every tasks and organize them all
together, by clarifying the dependencies.

A quick benchmark about existing Luigi use cases shows that machine learning
applications are extremely compatible with this package. That is particularly
true if we consider Map/Reduce frameworks. Here we can demonstrate that Luigi
keeps its interest in our case, with a slightly different usage.

Our workflow can be illustrated by the following figure, obtained with the help
of the Luigi daemon. It permits to explore the task pipeline graphically as
well as to explore their accomplishment degree while running.

#+CAPTION: Example of Luigi dependency graph
#+NAME: fig:luigi-dep-graph
[[./../figs/luigi_dependency_graph_example.png]]

We can identify some example of tasks in this graph:

- =OSMHistoryParsing= and =OSMTagParsing= are sources, these parsing tasks
  provide initial data sets by using =pyosmium= features.
- =OSMElementEnrichment= is an intermediary task in which additional variables
  are merged to OSM history data
- These additional variables are used in every metadata building tasks:
  =OSMTagMetaAnalysis=, =ElementMetadataExtract=, =ChangeSetMetadataExtract=
  and =UserMetadataExtract=.
- =MasterTask= is an abstract task that yields each final tasks. Its completion
  equals to the success of the pipelined procedure.

This example describes a part of our global framework for illustration purpose;
however this paper will only focus on parsing tasks. Additionnally, other tasks
could of course be put in the pipeline (*e.g.* machine-learning procedures to
classify users); they can be integrated in the framework with a minimal
effort.

* OSM data extraction

We saw in the last section that =osmium-tool= is needed for working with
specific OSM area. Here we provide an insight of the methodology with Bordeaux,
a medium-sized French city.

** Build our own OSM data sample

This method needs the area geographical coordinates. We recover them by drawing
the accurate bounding box within the OpenStreetMap [[https://www.openstreetmap.org/#map=10/45.0000/0.0000][API]] export tool. We get the
following bounding box coordinates: the top-left corner is at ={44.9335,
-0.7179}= whilst the bottom-right corner is at ={44.7216, -0.4134}=. These
coordinates seem unnecessarily precise, however they are just hand-made, by
successive zooms in the API.

#+CAPTION: Hand-made bounding box on Bordeaux city (France)
#+NAME: fig:osm-bb-example
#+attr_html: :width 800px
[[./../figs/osm_boundingbox_example.png]]

They are integrated in the following JSON configuration file, as well as the
output file name:

#+BEGIN_SRC js
{ "extracts": [ { 
              "output": "bordeaux-metropole.osh.pbf",
              "output_format": "osh.pbf",
              "description": "extract OSM history for Bordeaux (France)",
              "bbox": {"left": -0.7179, "right": -0.4134, "top": 44.9335, "bottom": 44.7216} } ],
              "directory": "/path/to/outputdir/"
}
#+END_SRC

A standard /.pbf/ file is build with this JSON configuration from the following
shell command:

#+BEGIN_SRC shell
osmium extract --with-history --config=region.json latest-planet.osh.pbf
#+END_SRC

Where =latest-planet.osh.pbf= is the input file (downloaded from Geofabrik
website, we still need some original data!). The =--with-history= flag here is
important to study the temporal evolution of some OSM entities, the number of
contributions, and check some specific OSM entities such as nodes, ways or
relations and get their history.

** Extract OSM data history

At this point, we have a /.pbf/ file that contains every OSM element versions
through time. We still have to write them into a /.csv/ file. That is the role
of =pyosmium=, evoked previously.

This operation can be done through a simple Python file (see snippets below).

#+BEGIN_SRC ipython :session osm :exports both
  import osmium as osm
  import pandas as pd

  class TimelineHandler(osm.SimpleHandler):
      def __init__(self):
          osm.SimpleHandler.__init__(self)
          self.elemtimeline = []

      def element(self, e, elem_type):
          self.elemtimeline.append([elem_type,
                                    e.id,
                                    e.version,
                                    e.visible,
                                    pd.Timestamp(e.timestamp),
                                    e.uid,
                                    e.changeset,
                                    len(e.tags)])

      def node(self, n):
          self.element(n, "node")

      def way(self, w):
          self.element(w, "way")

      def relation(self, r):
          self.element(r, "relation")
#+END_SRC

#+RESULTS:

First we have to import the useful libraries, that are =pandas= (to handle
dataframes and /.csv/ files) and =pyosmium=. Then, we define a small OSM data
handler, that saves every nodes, ways and relations into the =elemtimeline=
attribute (/i.e./ a list). We can observe that several attributes are recorded:
the element type, ID, version in the history, if it is currently visible on the
API, timestamp (when the current version has been set), user ID, change set ID
and the number of associated tags. These attributes are also available for ways
and relations, letting the chance to put a little more abstraction in this
class definition.

An instance of this class can be created so as to save OSM nodes within the
Bordeaux metropole area (see below). We pass the input file name to the
=apply_file= procedure, that scans the input file and fills the handler list
accordingly. After that we just have to transform the list into a pandas
DataFrame, to make further treatments easier.

#+BEGIN_SRC ipython :session osm :exports both
  tlhandler = TimelineHandler()
  tlhandler.apply_file("../src/data/raw/bordeaux-metropole.osh.pbf")
  colnames = ['type', 'id', 'version', 'visible', 'ts', 'uid', 'chgset', 'ntags']
  elements = pd.DataFrame(tlhandler.elemtimeline, columns=colnames)
  elements = elements.sort_values(by=['type', 'id', 'ts'])
  elements.head(10)
#+END_SRC

#+RESULTS:
#+begin_example
   type        id  version  visible                        ts    uid  chgset  \
0  node  21457126        2    False 2008-01-17 16:40:56+00:00  24281  653744   
1  node  21457126        3    False 2008-01-17 16:40:56+00:00  24281  653744   
2  node  21457126        4    False 2008-01-17 16:40:56+00:00  24281  653744   
3  node  21457126        5    False 2008-01-17 16:40:57+00:00  24281  653744   
4  node  21457126        6    False 2008-01-17 16:40:57+00:00  24281  653744   
5  node  21457126        7     True 2008-01-17 16:40:57+00:00  24281  653744   
6  node  21457126        8    False 2008-01-17 16:41:28+00:00  24281  653744   
7  node  21457126        9    False 2008-01-17 16:41:28+00:00  24281  653744   
8  node  21457126       10    False 2008-01-17 16:41:49+00:00  24281  653744   
9  node  21457126       11    False 2008-01-17 16:41:49+00:00  24281  653744   

   ntags  
0      0  
1      0  
2      0  
3      0  
4      0  
5      1  
6      0  
7      0  
8      0  
9      0  
#+end_example

The only thing to do here is to save the file into a /.csv/ file format:

#+BEGIN_SRC ipython :session osm :exports both
  elements.to_csv("bordeaux-metropole.csv", date_format='%Y-%m-%d %H:%M:%S')
#+END_SRC

At this point, the OSM data history is available in a /.csv/ format, coming
with a whole set of attributes that will be useful to describe the data. It
will give an opportunity to study the temporal evolution of the OSM API, as
developed in the next section.

* OSM API exploration through time

** A simple procedure to build dated OSM states

From the OSM data history we can recover the current state of OSM data (or more
precisely, the API state at the data extraction date). The only needed step is
to select the up-to-date OSM objects, /i.e./ those with the last existing
version, through a simple /groupby/ operation (by the way, it is equivalent to
download the corresponding /.osm/ file). As an extension, we propose a simple
but seminal procedure called =datedelems= that allows us to get the OSM API
picture given a specific date:

#+BEGIN_SRC ipython :session osm :exports both
  def datedelems(history, date): 
      datedelems = (history.query("ts <= @date")
                    .groupby(['type','id'])['version']
                    .max()
                    .reset_index())
      return pd.merge(datedelems, history, on=['type','id','version'])
#+END_SRC

#+RESULTS:

As a corollary we can build some time series aiming to describe the evolution
of the API in terms of OSM objects (nodes, ways, relations) or users. What if
we consider the API state month after month? What is the temporal evolution of
node, way, or relation quantities?

** How to get the OSM API evolution?

The following procedure helps us to describe the OSM API at a given date: how
many node/way/relation there are, how many users have contributed, how many
change sets have been opened:

#+BEGIN_SRC ipython :session osm :exports both
  def osm_stats(osm_history, timestamp): 
      osmdata = datedelems(osm_history, timestamp)
      nb_nodes = len(osmdata.query('type == "node"'))
      nb_ways = len(osmdata.query('type == "way"'))
      nb_relations = len(osmdata.query('type == "relation"'))
      nb_users = osmdata.uid.nunique()
      nb_chgsets = osmdata.chgset.nunique()
      return [nb_nodes, nb_ways, nb_relations, nb_users, nb_chgsets]
#+END_SRC

#+RESULTS:

We can obtain a dataframe that summarizes these statistics at regular
timestamps: monthly evaluations, or even finner analysis, by taking advantage
of pandas time series capabilities.

#+BEGIN_SRC ipython :session osm :exports both
  def osm_chronology(history, start_date, end_date, frequency="1M"):
      timerange = pd.date_range(start_date, end_date, freq=frequency).values
      osmstats = [osm_stats(history, str(date)) for date in timerange]
      osmstats = pd.DataFrame(osmstats, index=timerange,
                              columns=['n_nodes', 'n_ways', 'n_relations',
                                       'n_users', 'n_chgsets'])
      return osmstats
#+END_SRC

#+RESULTS:

These developments open further possibilities. Areas are comparable through
their history. A basic hypothesis could be: some areas have been built faster
than others, /e.g./ urban areas /vs/ desert areas. To investigate on the
evolutions of their OSM objects appears as a very appealing way to address this
issue!

** What about the Bordeaux area?

To illustrate the previous points, we can call the =osm_chronology= procedure
to Bordeaux-related OSM data. We can study the last 10 years, as an example:

#+BEGIN_SRC ipython :session osm :exports both
  osm_chronology(elements, "2007-01-01", "2017-01-01", "AS")  
#+END_SRC

#+RESULTS:
#+begin_example
            n_nodes  n_ways  n_relations  n_users  n_chgsets
2007-01-01        0       0            0        0          0
2008-01-01     4083     431            2       15         31
2009-01-01    76614   12429          129       85        778
2010-01-01   101978   15515          254      185       2005
2011-01-01   250197   40896          583      261       4212
2012-01-01   605777  104406          678      335       5482
2013-01-01  1761636  321662         1628      468       8353
2014-01-01  2166480  379577         3136      638      11121
2015-01-01  2269981  390201         4593      780      14452
2016-01-01  2302798  394903         5030      981      17379
2017-01-01  2349684  402942         5309     1203      21435
#+end_example

The figure below describes the evolution of nodes, ways and relations around
Bordeaux between 2007 and 2017, as well as the number of users and change
sets. The graphes are log-scaled, for a sake of clarity.

We can see that the major part of Bordeaux cartography has been undertaken
between fall of 2010 and spring of 2013, with a clear peak at the beginning
of 2012. This evolution is highly pronounced for nodes or even ways, whilst the
change set amount and the contributor quantity increased regularly. This may
denote the differences in terms of user behaviors: some of them create only a
few objects, while some others contributes with a large amount of created
entities.

#+CAPTION: Amount of OSM objects in the area of Bordeaux (France)
#+NAME: fig:bm-chronology
#+attr_html: :width 800px
[[./../figs/bordeaux-metropole-chronology-logscale.png]]

As a remark, the number of contributors plotted here is not really
representative of the total quantity of OSM contributors: we consider only
local data here. Active users all around the world are not those who have
collaborated for this specific region. These statistics are available by
exploiting the full-planet data set.

** Opening case study: comparing several french areas

Before concluding, here is provided a comparison between OSM node
amounts in several french areas. We just mention small areas, to keep the
evaluation short: Upper Normandy, a roughly rural environment with some
medium-sized cities (Rouen, Le Havre, Evreux...), Corsica, an montainous island
near to mainland France and French Guiana, an overseas area mainly composed of
jungle. The figure below shows the difference between these areas in terms of
OSM nodes and active contributors. To keep the comparison as faithful as
possible, we have divided these amounts by each surface area: respectively
12137, 8680 and 83534 square kilometers for Upper Normandy, Corsica and French
Guiana.

#+CAPTION: Amount of OSM nodes in several french areas
#+NAME: fig:multiarea-chronology-nodes
#+attr_html: :width 800px
[[./../figs/multiarea-chronology-weighted.png]]

Without any surprise, it is the mainland area (Upper Normandy) that is the most
dense on OSM. This area contains almost 700 nodes per square kilometer (quite
modest, however we talk about a rural area!). We can notice that they are
almost the same number of contributors between Normandy and Corsica. On the
other hand, French Guiana is an extrem example, as expected! There are less
than 15 nodes and 0.01 contributor per square kilometer.

* Conclusion

In this paper a first overview of OpenStreetMap data has been provided. On the
first sight exploiting these data is not so evident; however it appears simpler
with the convenient toolbox.

If the final target is to consider geospatial data quality, OSM data sets are a
very illustrative example: they are built collaboratively, by whoever who wants
to contribute. As a consequence, these data sets may be far from being
"perfect", even if this last notion is hard to define without any shared
reference data set.

Here a complete workflow has been described for getting OSM data, and loading
it into a Python environment. To do such operation a specific package has been
used: =pyosmium=. This tool, amongst others, allows to parse OSM data, and
handle it as =pandas= DataFrames so as to serialize it into classic /.csv/
files. Moreover a pipeline organizer completes this workflow, /i.e./
=Luigi=. Once the data are in the workspace, it becomes easier to provide some
insights about OSM API. As a first example, a small data set has been extracted
in the area of Bordeaux, France. Then a chronological study has showed how the
API evolves through time in terms of numbers of elements and active
contributors.

This first article gives the opportunity to describe some generic features
about OSM data, however the analysis has been bounded to seminal
contributions. This introduction appears as a pre-requisite before going
further. As a perspective a richer analysis focused on OSM data quality is
clearly targetted. One research question still emerges: what can we conclude
about data quality by exploiting OSM metadata, /i.e./ which impact change sets
have, how users contribute, how elements evolve time after time?

* References

- Arsanjani, J, Barron, C, Bakillah, M, Helbich, M. 2013. Assessing
  the quality of OpenStreetMap contributors together with their
  contributions. /Proceedings of the AGILE./ p14-17.
- Haklay, M. 2010. How good is volunteered geographical information? A
  comparative study of OpenStreetMap and Ordnance Survey datasets. /Environment
  and planning B: Planning and design./ 37(4), p.682-703.
- Helbich, M, Amelunxen, C, Neis, P, Zipf, A. 2012. Comparative
  spatial analysis of positional accuracy of OpenStreetMap and proprietary
  geodata. /Proceedings of GI Forum./ p.24-33.
- ISO. 2013. Geographic information: data
  quality. /ISO19157:2013./ Geneva, Switzerland: ISO.
- Koukoletsos, T, Haklay, M, Ellul, C. 2011. An automated method to
  assess data completeness and positional accuracy of
  OpenStreetMap. /GeoComputation./ 3, p.236-241.
- Neis, P, Zipf, A. 2012. Analyzing the contributor activity of a
  volunteered geographic information project: the case of OpenStreetMap. /ISPRS
  International Journal of Geo-Information, Molecular Diversity Preservation./
  1, p.146-165.
- Van Oort, P. 2006. Spatial data quality: from description to
  application. /PhD report./ Wageningen Universiteit.
