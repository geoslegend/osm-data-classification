#+TITLE: A data-oriented framework to assess OSM data quality (part 2): evaluate element quality by clustering contributors
#+AUTHOR: RaphaÃ«l Delhome <raphael.delhome@oslandia.com>, Damien Garaud <damien.garaud@oslandia.com>, Hugo Mercier <hugo.mercier@oslandia.com>

* Introduction

At Oslandia, we like working with Open Source tool projects and handling Open
(geospatial) Data. In this article, we will play with [[https://www.openstreetmap.org/][OpenStreetMap]] (/OSM/) and
the subsequent data.

After a first article dedicated to the framework presentation and to data
extraction, this second article continues to deliver the methodology that links
OSM data history and quality assessment.

We have seen in the first article (Garaud /et al./; 2017) how to parse
OpenStreetMap data, and how to integrate it in a Python workflow. This second
paper aims at exploiting the parsed data in machine learning procedures to
produce groups of similar contributors; these groups allowing us to hypothesize
on data quality.

In this article, we will first produce contributor-focused information from
data history, /i.e./ user metadata. Then we will design an unsupervised
learning procedure to group users by using classical machine learning
algorithms. In the last section of the paper we will conclude about OSM data
quality by producing new maps.

* User metadata production

As the extraction of OSM data history has already been done in the previous
article, we consider here that we have a =elements= table that represents the
history. As an example, we still exploit the area of Bordeaux.

#+BEGIN_SRC ipython :session osm :exports none
import pandas as pd
elements = pd.read_csv("../src/data/output-extracts/bordeaux-metropole/element.csv", parse_dates=['ts'], index_col=0)
#+END_SRC

#+RESULTS:

We could compare the OSM data with some alternative data source, however there
is no real consensus about the perfectness of any alternative data set. Knowing
that, we choose to analyse metadata associated to OSM elements instead of data
themselves.

Some data agregation operations are needed in this way, to better describe the
evolution of the OSM objects and its contributors.

** Metadata definition

We plan to go deeper than just consider the geometric data: some additional
information is available if we check the history of contributions. There are
three main cases to define.

+ Element metadata: that sounds trivial to consider such data, however it is
  too descriptive to provide any add-on in terms of quality assessment, and it
  is fairly intensive in terms of computing resources (~3millions only for a
  medium city like Bordeaux);
+ Changeset metadata: each changeset is characterized by a number of
  modifications (creations, improvements, deletions), one may distinguish
  "productive" changesets, where a large amount of elements are modified, and
  where modifications are durable;
+ User metadata: each user may be productive or not, depending on the number of
  modifications he makes, the amount of time its modifications stay valid, and
  the diversity of modified elements.

The information may be gathered more efficiently by considering the last
possibility: the contributors themselves. We hypothesize that we can
characterize OSM data quality by considering which type of user contributes the
most to each node, way or relation. In a more simple way, we can consider the
most experienced user who've contributed on an element as a flag about the
element quality. The quality of an element may also be indicated by the type
(more or less experienced) of its last contributor. This last hypothesis will
be our central thread in the next section.

** Metadata extraction

*** Time-related features

We begin with the time users have spent on OSM.

#+BEGIN_SRC ipython :session osm :exports code
    user_md = (elements.groupby('uid')['ts']
                .agg(["min", "max"])
                .reset_index())
    user_md.columns = ['uid', 'first_at', 'last_at']
    user_md['lifespan'] = ((user_md.last_at - user_md.first_at)
                            / pd.Timedelta('1d'))
    extraction_date = elements.ts.max()
    user_md['n_inscription_days'] = ((extraction_date - user_md.first_at)
                                      / pd.Timedelta('1d'))
    elements['ts_round'] = elements.ts.apply(lambda x: x.round('d'))
    user_md['n_activity_days'] = (elements
                                  .groupby('uid')['ts_round']
                                  .nunique()
                                  .reset_index())['ts_round']
    user_md.sort_values(by=['first_at'])
#+END_SRC

With these short code lines, we have gathered some temporal features in order
to know how each user contributes through time. As an example, the user with
the /ID=4074141/ is registered as an OSM contributor for 258 days; its lifespan
on the OSM website is almost 3 days; he made modifications at two different
days.

*** Changeset-related features

Then we can focus on changeset-related information. By definition, each user
have opened at least one changeset. We can construct a small changeset metadata
DataFrame:

#+BEGIN_SRC ipython :session osm :exports both
  chgset_md = (elements.groupby('chgset')['ts']
                .agg(["min", "max"])
                .reset_index())
  chgset_md.columns = ['chgset', 'first_at', 'last_at']
  chgset_md['duration'] = ((chgset_md.last_at - chgset_md.first_at)
                            / pd.Timedelta('1m'))
  chgset_md = pd.merge(chgset_md,
                       elements[['chgset','uid']].drop_duplicates(),
                       on=['chgset'])
  chgset_md.sample(1).T
#+END_SRC

#+RESULTS:
:                         23235
: chgset               29244222
: first_at  2015-03-04 00:00:00
: last_at   2015-03-04 00:00:00
: duration                    0
: uid                   2179792

Each changeset is associated with its starting and ending time, its duration
(in minute) and the responsible user. We then may associate a changeset
quantity and mean duration time for each user.

#+BEGIN_SRC ipython :session osm :exports code
  user_md['n_chgset'] = (chgset_md.groupby('uid')['chgset']
                         .count()
                         .reset_index())['chgset']
  user_md['dmean_chgset'] = (chgset_md.groupby('uid')['duration']
                             .mean()
                             .reset_index())['duration']
#+END_SRC

To continue with the previous example the user 4074141 had produced three
changesets during its lifespan, and the mean duration of these changesets is
around 22 minutes.

*** Contribution intensity

Then we observed on some preliminary observation that some users were so
productive that they modify some elements several times; a typical bot-like
behavior if this amount is large, or a simple auto-corrections? We can add this
information as follows:

#+BEGIN_SRC ipython :session osm :exports code
    contrib_byelem = (elements.groupby(['elem', 'id', 'uid'])['version']
                      .count()
                      .reset_index())
    user_md['nmean_modif_byelem'] = (contrib_byelem.groupby('uid')['version']
                                     .mean()
                                     .reset_index())['version']
#+END_SRC

The user 4074141 seems to modify each OSM elements almost three times. That's
quite few to conclude to its bot nature, however he seems quite unsure about
his contribution...

*** Element-related features

In order to characterize how the user contributes, a lot of additional features
are still missing. The most important ones are related to the amount of
modifications.

#+BEGIN_SRC ipython :session osm :exports code
    newfeature = (elements.groupby(['uid'])['id']
                  .count()
                  .reset_index()
                  .fillna(0))
    newfeature.columns = ['uid', "n_total_modif"]
    user_md = pd.merge(user_md, newfeature, on='uid', how="outer").fillna(0)
    newfeature = (elements.query('elem == "node"').groupby(['uid'])['id']
                  .count()
                  .reset_index()
                  .fillna(0))
    newfeature.columns = ['uid', "n_total_modif_node"]
    user_md = pd.merge(user_md, newfeature, on='uid', how="outer").fillna(0)
    newfeature = (elements.query('elem == "way"').groupby(['uid'])['id']
                  .count()
                  .reset_index()
                  .fillna(0))
    newfeature.columns = ['uid', "n_total_modif_way"]
    user_md = pd.merge(user_md, newfeature, on='uid', how="outer").fillna(0)
    newfeature = (elements.query('elem == "relation"').groupby(['uid'])['id']
                  .count()
                  .reset_index()
                  .fillna(0))
    newfeature.columns = ['uid', "n_total_modif_relation"]
    user_md = pd.merge(user_md, newfeature, on='uid', how="outer").fillna(0)
#+END_SRC

The previous user is very active to map the Bordeaux area: he proposed 1832
modifications, amongst which 1783, 46 and 3 were respectively dedicated to
nodes, ways and relations. However the amount of modified elements should be
smaller, as this user made several contributions per element, on average.

*** Modification-related features

The number of modifications can be described even more finely. Why don't we
consider if modifications are still valid, or if other modifications arise
after the user action? What about elements that have been deleted since (we
consider than working on a useless element is not so valuable for the
community)?

We need associating a little bit more features to OSM elements. Is the current
version an initialization of the object? Is it the up-to-date version? Will it
be corrected (by an alternative user or the current user himself)?

#+BEGIN_SRC ipython :session osm :exports both
    import numpy as np

    osmelem_versioning = (elements.groupby(['elem', 'id'])['version']
                .agg(["first", "last"])
                .reset_index())
    osmelem_versioning.columns = ['elem', 'id', 'vmin', 'vmax']

    elements = pd.merge(elements, osmelem_versioning, on=['elem', 'id'])
    elements['init'] = elements.version == elements.vmin
    elements['up_to_date'] = elements.version == elements.vmax
    # note that the 'elements' DataFrame have been sorted by type, id, ts
    elements['willbe_corr'] = np.logical_and(elements.id.diff(-1) == 0,
                                             elements.uid.diff(-1) != 0)
    elements['willbe_autocorr'] = np.logical_and(elements.id.diff(-1) == 0,
                                                 elements.uid.diff(-1) == 0)

#+END_SRC

#+RESULTS:

These features help to describe more precisely the user contributions:

#+BEGIN_SRC ipython :session osm :exports both
def create_count_features(metadata, element_type, data, grp_feat, res_feat, feature_suffix):
    feature_name = 'n_'+ element_type + '_modif' + feature_suffix
    newfeature = (data.groupby([grp_feat])[res_feat]
                  .count()
                  .reset_index()
                  .fillna(0))
    newfeature.columns = [grp_feat, feature_name]
    metadata = pd.merge(metadata, newfeature, on=grp_feat, how="outer").fillna(0)
    return metadata

def extract_modif_features(metadata, data, element_type):
    typed_data = data.query('elem==@element_type')
    metadata = create_count_features(metadata, element_type, typed_data,
                               'uid', 'id', '')
    metadata = create_count_features(metadata, element_type,
                               typed_data.query("init"),
                               'uid', 'id', "_cr")
    metadata = create_count_features(metadata, element_type,
                               typed_data.query("not init and visible"),
                               'uid', 'id', "_imp")
    metadata = create_count_features(metadata, element_type,
                               typed_data.query("not init and not visible"),
                               'uid', 'id', "_del")
    metadata = create_count_features(metadata, element_type,
                               typed_data.query("up_to_date"),
                               'uid', 'id', "_utd")
    metadata = create_count_features(metadata, element_type,
                               typed_data.query("willbe_corr"),
                               'uid', 'id', "_cor")
    metadata = create_count_features(metadata, element_type,
                               typed_data.query("willbe_autocorr"),
                               'uid', 'id', "_autocor")
    return metadata

user_md = extract_modif_features(user_md, elements, 'node')
user_md = extract_modif_features(user_md, elements, 'way')
user_md = extract_modif_features(user_md, elements, 'relation')
user_md = user_md.set_index('uid')
user_md.query("uid == 4074141").T
#+END_SRC

#+RESULTS:
#+begin_example
uid                                   4074141
lifespan                                    3
n_inscription_days                        258
n_activity_days                             2
n_chgset                                    3
dmean_chgset                                0
nmean_modif_byelem                    2.94061
n_total_modif                            1832
n_total_modif_node                       1783
n_total_modif_way                          46
n_total_modif_relation                      3
n_node_modif                             1783
n_node_modif_cr                             0
n_node_modif_imp                         1783
n_node_modif_del                            0
n_node_modif_utd                            0
n_node_modif_cor                          598
n_node_modif_autocor                     1185
n_way_modif                                46
n_way_modif_cr                              0
n_way_modif_imp                            46
n_way_modif_del                             0
n_way_modif_utd                             0
n_way_modif_cor                            23
n_way_modif_autocor                        23
n_relation_modif                            3
n_relation_modif_cr                         0
n_relation_modif_imp                        3
n_relation_modif_del                        0
n_relation_modif_utd                        0
n_relation_modif_cor                        2
n_relation_modif_autocor                    1
#+end_example

Amongst the 1783 modifications on node, there are 1783 improvements (so, no
creation, no deletion). 598 of these modifications have been corrected by other
users, and 1185 of them refer to auto-corrections; but no node modification
result in up-to-date node. We can draw a comparable picture for ways and
relations. As a result, we have identified a user that contributes a lot to
improve OSM elements; however his contributions are never enough to complete
the element representation.

We can also add some information about the OSM editors used by each contributor,
not shown here for a sake of concision.

#+BEGIN_SRC ipython :session osm :exports none
user_md = pd.read_csv("../src/data/output-extracts/bordeaux-metropole/user-metadata-extra.csv", index_col=0)
#+END_SRC

By considering every single user that has contributed on a given area, we can
easily imagine that some groups could arise.

* Unsupervised learning with user metadata

In the last section, we have seen that user metadata can be easily built by
some agregation operations starting from OSM data history. We have proposed a
bunch of features to characterize as well as possible the way people
contributes to OSM. As a total, we have 40 features that describe user
behavior, and 2073 users.

In the current section, we will see how to use this metadata to group OSM
users, with the help of some machine learning well-known procedures.

** User metadata transformation

We can't proceed directly to machine learning procedures: they need
gaussian-distributed features as input. As illustrated by the following
histograms, focused on some available features, it is not the case here;
moreover the features are highly-skewed, that leading us to consider an
alternative normalization scheme.

#+CAPTION: Histogram of node, way and relation modification amounts around Bordeaux
#+NAME: bm_sk_hist
[[file:../figs/bordeaux-metropole-skewed-histograms.png]]

The basic idea we want you to keep in mind is the following one: if we find
some mathematical tricks to express our variables between simple bounds (/e.g./
0 and 100, or -1 and 1), we could have smarter way to represent the user
characteristics.

First of all you should notice that a lot of variables can be expressed as
percentages of other variables:

- the number of node/way/relation modifications amongst all modifications;
- the number of created/improved/deleted elements amongst all modifications,
  for each element type;
- the number of changesets opened with a given editor, amongst all changesets.

#+BEGIN_SRC ipython :session osm :exports none
def normalize_features(metadata, total_column):
    transformed_columns = metadata.columns[metadata.columns.to_series()
                                           .str.contains(total_column)]
    metadata[transformed_columns[1:]] = metadata[transformed_columns].apply(lambda x: (x[1:]/x[0]).fillna(0), axis=1)

normalize_features(user_md, 'n_total_modif')
normalize_features(user_md, 'n_node_modif')
normalize_features(user_md, 'n_way_modif')
normalize_features(user_md, 'n_relation_modif')
normalize_features(user_md, 'n_total_chgset')
#+END_SRC

#+RESULTS:

Other features can be normalized starting from their definition: we know that
=lifespan= and =n_inscription_days= can't be larger than the OSM lifespan
itself (we consider the OSM lifespan as the difference between the first year
of modification within the area and the extraction date).

#+BEGIN_SRC ipython :session osm :exports none
timehorizon = (pd.Timestamp("2017-02-19") - pd.Timestamp("2007-01-01"))/pd.Timedelta('1d')
user_md['lifespan'] = user_md['lifespan'] / timehorizon
user_md['n_inscription_days'] = user_md['n_inscription_days'] / timehorizon
#+END_SRC

#+RESULTS:

Finally we have to consider the remainder of features that can't be normalized
as percentages of other variables, or as percentages of meaningful
quantities. In such cases we compare users between each other: knowing that a
user did =N= modifications is interesting, however it tells nothing about the
amount of users that are less productive. That's typically the definition of
the empirical cumulative distribution function.

#+BEGIN_SRC ipython :session osm :exports none
import statsmodels.api as sm

def ecdf_transform(metadata, feature):
    ecdf = sm.distributions.ECDF(metadata[feature])
    metadata[feature] = ecdf(metadata[feature])
    new_feature_name = 'u_' + feature.split('_', 1)[1]
    return metadata.rename(columns={feature: new_feature_name})

user_md = ecdf_transform(user_md, 'n_activity_days')
user_md = ecdf_transform(user_md, 'n_chgset')
user_md = ecdf_transform(user_md, 'nmean_modif_byelem')
user_md = ecdf_transform(user_md, 'n_total_modif')
user_md = ecdf_transform(user_md, 'n_node_modif')
user_md = ecdf_transform(user_md, 'n_way_modif')
user_md = ecdf_transform(user_md, 'n_relation_modif')
user_md = ecdf_transform(user_md, 'n_total_chgset')
#+END_SRC

After transforming our features we then know that the user with ID 4074141 did
more node, way and relation modifications than respectively 97.3, 2.5 and 0.2%
of other users, or that amongst his node modifications, 100% were improvements,
and so on...

In order to complete the normalization procedure, we add a final step that
consists in scaling the features, to ensure that all of them have the same
/min/ and /max/ values. As the features are still skewed, we do it according to
a simple Min-Max rule, so as to avoid too much distorsion of our data:

#+BEGIN_SRC ipython :session osm :exports code
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler(quantile_range=(0.0,100.0))
X = scaler.fit_transform(user_md.values)
#+END_SRC

** Develop a Principle Component Analysis (PCA)

Reduce the dimensionality of a problem often appears as a unavoidable
pre-requisite before undertaking any classification effort.

As developped previously, we have 40 variables. That seems quite small for
implementing a PCA (we could apply directly a clustering algorithm on our
normalized data); however for a sake of clarity regarding result
interpretation, we decide to add this step into the analysis.

*** PCA design

The principle component analysis is a linear projection of individuals on a
smaller dimension space. It provides uncorrelated components, dropping
redundant information given by subsets of initial dataset.

Actually there is no ideal component number, it can depend on modeller wishes;
in general this quantity is chosen according to the explained variance
proportion, and/or according to eigen values of components. There are some rule
of thumbs for such a situation: we can choose to take components to cover at
least 70% of the variance, or to consider components that have an eigen value
larger than 1.

#+CAPTION: User metadata variance analysis and ideal number of PCA components
#+NAME: bm_varmat
#+ATTR_HTML: :width 100px
[[../figs/bordeaux-metropole-varmat.png]]

Here the second rule of thumb fails, as we do not use a standard scaling
process (/e.g./ less mean, divided by standard deviation), however the first
one makes us consider 6 components (that explain around 72% of the total
variance).

*** PCA running

The PCA algorithm is loaded from a =sklearn= module, we just have to run it by
giving a number of components as a parameter, and to apply the =fit_transform=
procedure to get the new linear projection. Moreover the contribution of each
feature to the new components is straightforwardly accessible with the
=sklearn= API.

#+BEGIN_SRC ipython :session osm :exports both
from sklearn.decomposition import PCA

model = PCA(n_components=6)
Xpca = model.fit_transform(X)
pca_cols = ['PC' + str(i+1) for i in range(6)]
pca_ind = pd.DataFrame(Xpca, columns=pca_cols, index=user_md.index)
pca_var = pd.DataFrame(model.components_, index=pca_cols,
                       columns=user_md.columns).T
pca_ind.query("uid==4074141").T
#+END_SRC

#+RESULTS:
: uid   4074141
: PC1 -0.117667
: PC2  1.145473
: PC3  0.272944
: PC4 -0.095750
: PC5 -0.151553
: PC6  0.932512

After running the PCA, the information about the user is summarized with these
6 values. It could be largely better to know which meaning these 6 components
have.

*** Component interpretation

The feature contributions to each components are comprised between -1 (a strong
negative contribution) and 1 (a strong positive contribution). Additionnally
there is a mathematical relation between all contributions to a given
component: the sum of squares equals to 1. As a consequence we can really
consider that features can be ranked by order of importance in the component
definition. These contributions are plotted in the figure [[bm_feature_contrib]].

#+CAPTION: Feature contribution to each PCA components
#+NAME: bm_feature_contrib
#+ATTR_HTML: :width 100px
[[../figs/bordeaux-metropole-feature-contrib.png]]

Here our six components may be described as follows:

+ PC1 (28.5% of total variance) is really impacted by relation modifications,
  this component will be high if user did a lot of relation improvements (and
  very few node and way modifications), and if these improvements have been
  corrected by other users since. It is the sign of an specialization to
  complex structures. This component also refers to contributions from foreign
  users (/i.e./ not from the area of interest, here the Bordeaux area),
  familiar with /JOSM/.
+ PC2 (14.5% of total variance) characterizes how experienced and versatile are
  users: this component will be high for users with a high number of activity
  days, a lot of local as well as total changesets, and high numbers of node,
  way and relation modifications. This second component highlights /JOSM/ too.
+ PC3 (9.1% of total variance) describes way-focused contributions by old users
  (but not really productive since their inscription). A high value is
  synonymous of corrected contributions, however that's quite mechanical: if
  you contributed a long time ago, your modifications would probably not be
  up-to-date any more. This component highlights /Potlatch/ and /JOSM/ as the
  most used editors.
+ PC4 (8.7% of total variance) looks like PC3, in the sense that it is strongly
  correlated with way modifications. However it will concern newer users: a
  more recent inscription date, contributions that are less corrected, and more
  often up-to-date. As the preferred editor, this component is associated with
  /iD/.
+ PC5 (6.9% of total variance) refers to a node specialization, from very
  productive users. The associated modifications are overall improvements that
  are still up-to-date. However, PC5 is linked with users that are not at ease
  in our area of interest, even if they produced a lot of changesets
  elsewhere. /JOSM/ is clearly the corresponding editor.
+ PC6 (4.8% of total variance) is strongly impacted by node improvements, by
  opposition to node creations (a similar behavior tends to emerge for
  ways). This less important component highlights local specialists: a fairly
  high quantity of local changesets, but a small total changeset
  quantity. Like for PC4, the editor used for such contributions is /iD/.

*** Describe individuals positioning after dimensionality reduction

From the previous lightings, we can recall the example of user 4074141.

This user is really experienced (high value of PC2), even if this experience
tends to be local (negative value for PC5). The fairly good value for PC6
enforces the hypothesis credibility. We can imagine that the user quite
versatile (PC2), with a specialty on node improvements (PC6).

Even if this interpretation exercise may look quite abstract, the comparison
between this interpretation and the description in the first section looks
satisfying.

** Cluster the user starting from their past activity

At this point, we have a set of active users (those who have contributed to the
focused area). We propose now to classify each of them without any knowledge on
their identity or experience with geospatial data or OSM API, by the way of
unsupervised learning. Indeed we will design clusters with the k-means
algorithm, and the only inputs we have are the synthetic dimensions given by
the previous PCA. These dimensions contain information about the past
contributions of each user.

Recall that we are investigating on OSM data quality, it is quite hard to have
an absolute answer, especially without any trustworthy "ground truth". Here we
hypothesize that typical groups of users (/e.g./ beginners, intermediate,
advanced, experts...) will arise from the classification algorithm.

*** k-means design: how many cluster may we expect from the OSM metadata?

Like for the PCA, the k-means algorithm is characterized by a parameter that
must be tuned, /i.e./ the cluster number.

#+CAPTION: Optimal cluster number regarding elbow and silhouette methods
#+NAME: bm_cluster_nb
#+ATTR_HTML: :width 100px
[[../figs/bordeaux-metropole-cluster-number.png]]

How many clusters can be identified? We only have access to soft
recommendations given by state-of-the-art procedures. As an illustration here,
we use elbow method, and clustering silhouette.

The former represents the intra-cluster variance, /i.e./ the sparsity of
observations within clusters. It obviously decreases when the cluster number
increases. To keep the model simple and do not overfit it, this quantity has to
be as small as possible. That's why we evoke an "elbow": we are looking for a
bending point designing a drop of the explained variance marginal gain. The
latter is a synthetic metric that indicates how well each individuals is
represented by its cluster. It is comprised between 0 (bad clustering
representation) and 1 (perfect clustering).

The first criterion suggests to take either 2 or 6 clusters, whilst the second
criterion is larger with 6 or 7 clusters. We then decide to take on 6 clusters.

*** k-means running: OSM contributor classification

How to interpret the six chosen clusters starting from the Bordeaux area
dataset?

#+BEGIN_SRC ipython :session osm :exports both
from sklearn.cluster import KMeans

model = KMeans(n_clusters=6, n_init=100, max_iter=1000)
kmeans_ind = pca_ind.copy()
kmeans_ind['Xclust'] = model.fit_predict(pca_ind.values)
kmeans_centroids = pd.DataFrame(model.cluster_centers_,
                                columns=pca_ind.columns)
kmeans_centroids['n_individuals'] = (kmeans_ind
                                     .groupby('Xclust')
                                     .count())['PC1']
kmeans_centroids
#+END_SRC

#+RESULTS:
:         PC1       PC2       PC3       PC4       PC5       PC6  n_individuals
: 0 -0.109548  1.321479  0.081623  0.010538  0.117814 -0.024912            317
: 1 -0.901269  0.034718  0.594164 -0.395584 -0.323118 -0.167048            272
: 2 -1.077956  0.027944 -0.595774  0.365218 -0.005808 -0.022253            353
: 3 -0.345311 -0.618197  0.842708  0.872656  0.180983 -0.004831            228
: 4  1.509024 -0.137856 -0.142928  0.032838 -0.120928 -0.031581            585
: 5 -0.451754 -0.681200 -0.269507 -0.763645  0.258082  0.253980            318

The k-means algorithm makes six relatively well-balanced groups (the group 4 is
larger than the others, however the difference is not so high):

+ Group 0 (15.3% of users): high positive PC2 value, other components are
  closed to 0; this group represents most experienced and versatile users. The
  users are seen as OSM key contributors.
+ Group 1 (13.2% of users): medium negative PC1 value, small positive PC3
  value, small negative PC4 and PC5 values; this cluster refers to old one-shot
  contributors, mainly interested in way modifications.
+ Group 2 (17.0% of users): medium negative PC1 value, small negative PC3
  value, small positive PC4 value; this category of user is very close to the
  previous one, the difference being the more recent period during which they
  have contributed.
+ Group 3 (11.0% of users): medium positive PC3 and PC4 values, small negative
  PC1 and PC2 values; this user cluster contains contributors that are locally
  unexperienced, they have proposed mainly way modifications.
+ Group 4 (28.2% of users): high positive PC1 value, other components are
  closed to 0; this cluster refers to relation specialists, users that are
  fairly productive on OSM.
+ Group 5 (15.3% of users): medium negative PC1, PC2 and PC4 values, small
  negative PC3 value, small positive PC5 and PC6 values; this last cluster
  gathers very unexperienced users, that comes just a few times on OSM to
  modify mostly nodes.

To complete this overview, we can plot individuals according to their group,
with respect to the most important components:

#+CAPTION: Clustered individuals positionings regarding PCA components
#+NAME: bm_cluster_indiv_plot
#+ATTR_HTML: :width 100px
[[../figs/bordeaux-metropole-kmeans-plot.png]]

The first two components allow to discriminate clearly C0 and C4. We need the
third and the fourth components to differentiate C1 and C2 on the first hand,
and C3 and C5 on the other hand. The last two components do not provide any
additional information.

This user classification has been produced without any preliminar
knowledge about who they are, and which skills they have. That's an
illustration of the power of unsupervised learning; we will try to apply this
clustering in OSM data quality assessment in the next section.

* Data quality visualisation
** Description of OSM element

*** Element metadata extraction

Produce valuable information about quality need focusing on element metadata
itself. What is an OSM element? How to extract its associated metadata? This
part is relatively similar to the job already done with users.

We know from previous analysis that an element is created during a changeset by
a given contributor, may be modified several times by whoever, and may be
deleted as well. This kind of object may be either a "node", a "way" or a
"relation". We also know that there may be a set of different tags associated
with the element. Of course the list of every operations associated to each
element is recorded in the OSM data history.

#+BEGIN_SRC ipython :session osm :exports none
import pandas as pd

elements = pd.read_table('../src/data/output-extracts/bordeaux-metropole/bordeaux-metropole-elements.csv', parse_dates=['ts'], index_col=0, sep=",")
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session osm :exports none
elem_md = (elements.groupby(['elem', 'id'])['ts']
            .agg(["min", "max"])
            .reset_index())
elem_md.columns = ['elem', 'id', 'first_at', 'last_at']
elem_md['lifespan'] = (elem_md.last_at - elem_md.first_at)/pd.Timedelta('1D')
extraction_date = elements.ts.max()
elem_md['n_days_since_creation'] = ((extraction_date - elem_md.first_at)
                                  / pd.Timedelta('1d'))
elem_md['n_days_of_activity'] = (elements
                              .groupby(['elem', 'id'])['ts']
                              .nunique()
                              .reset_index())['ts']
elem_md = elem_md.sort_values(by=['first_at'])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session osm :exports results
    elem_md['version'] = (elements.groupby(['elem','id'])['version']
                          .max()
                          .reset_index())['version']
    elem_md['n_chgset'] = (elements.groupby(['elem', 'id'])['chgset']
                           .nunique()
                           .reset_index())['chgset']
    elem_md['n_user'] = (elements.groupby(['elem', 'id'])['uid']
                         .nunique()
                         .reset_index())['uid']
    osmelem_last_user = (elements
                         .groupby(['elem','id'])['uid']
                         .last()
                         .reset_index())
    osmelem_last_user = osmelem_last_user.rename(columns={'uid':'last_uid'})
    elements = pd.merge(elements, osmelem_last_user,
                       on=['elem', 'id'])
    elem_md = pd.merge(elem_md,
                       elements[['elem', 'id', 'version', 'visible', 'last_uid']],
                       on=['elem', 'id', 'version'])
    elem_md = elem_md.set_index(['elem', 'id'])
    elem_md.query("id == 1669353159").T
#+END_SRC

#+RESULTS:
#+begin_example
elem                                  node
id                              1669353159
first_at               2012-03-10 00:00:00
last_at                2012-03-10 00:00:00
lifespan                                 0
n_days_since_creation                 1807
n_days_of_activity                       1
version                                  1
n_chgset                                 1
n_user                                   1
visible                               True
last_uid                            219843
#+end_example

As an illustration we have above an old one-versionned node, still visible on
the OSM website.

*** Characterize OSM elements with user classification

This set of features is only descriptive, more information is needed to
characterize OSM data quality. That is the moment to exploit the user
classification produced in the previous section.

As a recall, we hypothesized that clustering the users permits to evaluate
their trustworthiness as OSM contributors. They are either beginners, or
intermediate users, or even OSM experts.

Each OSM entity may have received one or more contributions by users of each
group. Let's say the entity quality is good if its last contributor is
experienced. That leads us to classify the OSM entities themselves in return.

#+BEGIN_SRC ipython :session osm :exports results
user_groups = pd.read_hdf("../src/data/output-extracts/bordeaux-metropole/bordeaux-metropole-user-kmeans.h5", "/individuals")
user_groups.query("uid == 4074141").T
#+END_SRC

#+RESULTS:
: uid      4074141
: PC1    -0.117667
: PC2     1.145494
: PC3     0.272941
: PC4    -0.095759
: PC5    -0.151579
: PC6     0.932229
: Xclust  2.000000

The appropriate cluster for each user is saved into the column =Xclust=, it can
be joined to the remainder of element metadata as follows:

#+BEGIN_SRC ipython :session osm :exports both
    elem_md = elem_md.join(user_groups.Xclust, on='last_uid')
    elem_md = elem_md.rename(columns={'Xclust':'last_uid_group'})
    elem_md.reset_index().to_csv("../src/data/output-extracts/bordeaux-metropole/bordeaux-metropole-element-metadata.csv")
    elem_md.query("id == 1669353159")[["last_uid", "last_uid_group"]].T
#+END_SRC

#+RESULTS:
: elem                 node
: id             1669353159
: last_uid           219843
: last_uid_group          2

*** Recover the geometry information

Another OSM data parser has been used to retrieve OSM element geometries,
/i.e./ =osm2pgsql=. By assuming the existence of a =osm= database, owned by
=user= and a file =bordeaux-metropole.osm.pbf=:

#+BEGIN_SRC sh
osm2pgsql -E 27572 -d osm -U rde -p bordeaux_metropole --hstore --extra-attributes /home/rde/data/osm-history/raw/bordeaux-metropole.osm.pbf
#+END_SRC

#+RESULTS:

A France-focused SRID (27572) and a prefix for naming output databases =point=,
=line=, =polygon= and =roads= are specified as well.

Focusing on the =line= subset, that contains the physical roads, among other
structures (it roughly corresponds to the OSM ways), we want to build an
enriched version of element metadata, with geometries.

First we can create the table =bordeaux_metropole_geomelements=, that will
contain our metadata...

#+BEGIN_SRC sql :engine postgresql :cmdline "-U rde -d osm" :exports code
DROP TABLE IF EXISTS bordeaux_metropole_elements;
DROP TABLE IF EXISTS bordeaux_metropole_geomelements;
CREATE TABLE bordeaux_metropole_elements(
       id int,
       elem varchar,
       osm_id bigint,
       first_at varchar,
       last_at varchar,
       lifespan float,
       n_days_since_creation float,
       n_days_of_activity float,
       version int,
       n_chgsets int,
       n_users int,
       visible boolean,
       last_uid int,
       last_user_group int
);
#+END_SRC

#+RESULTS:
| DROP TABLE   |
|--------------|
| DROP TABLE   |
| CREATE TABLE |

...then, populate it with the accurate =.csv= file...

#+BEGIN_SRC sql :engine postgresql :cmdline "-U rde -d osm" :exports code
COPY bordeaux_metropole_elements
FROM '/home/rde/data/osm-history/output-extracts/bordeaux-metropole/bordeaux-metropole-element-metadata.csv'
WITH(FORMAT CSV, HEADER, QUOTE '"');
#+END_SRC

#+RESULTS:
| COPY 2760999 |
|--------------|

...and finally, merge the metadata with the geometry data gathered with
=osm2pgsql=.

#+BEGIN_SRC sql :engine postgresql :cmdline "-U rde -d osm" :exports code
DROP TABLE IF EXISTS bordeaux_metropole_geomelements;
SELECT l.osm_id, h.lifespan, h.n_days_since_creation,
h.version, h.visible, h.n_users, h.n_chgsets,
h.last_user_group, l.way AS geom
INTO bordeaux_metropole_geomelements
FROM bordeaux_metropole_elements as h
INNER JOIN bordeaux_metropole_line as l
ON h.osm_id = l.osm_id AND h.version = l.osm_version
WHERE l.highway IS NOT NULL AND h.elem = 'way'
ORDER BY l.osm_id;
#+END_SRC

#+RESULTS:
| DROP TABLE   |
|--------------|
| SELECT 29349 |

From now, we can use the last contributor cluster as an additional information
to generate maps, so as to study data quality.

** Quality assessing through map production

If each OSM entities (/e.g./ roads) can be characterized, then we can draw
quality maps by highlighting the most trustworthy ones, as well as those with
which we have to stay cautious.

In this section we will continue to focus on roads within the Bordeaux
area. The different maps will be produced with the help of Qgis.

*** Simple metadata plotting

As a first insight on OSM elements, we can plot each OSM ways regarding a
simple features, for instance the number of users who have contributed.

#+CAPTION: Number of active contributors per OSM way in Bordeaux
#+NAME: fig:bm_nusers
#+ATTR_HTML: width="100px"
[[../figs/bordeaux-metropole-nb-users-100dpi.png]]

With this map, we see that the ring around Bordeaux is the most intensively
modified part of the road network: more contributors are implied in the way
completion. Some major roads within the city center that present the same
characteristics.

*** Representation of OSM elements with respect to quality

A similar map may be designed with the user classification information:

#+CAPTION: OSM roads around Bordeaux, according to the last user cluster (1: C1, relation experts; 2: C0, versatile expert contributors; 3: C4, recent one-shot way contributors; 4: C3, old one-shot way contributors; 5: C5, locally-unexperienced way specialists)
#+NAME:   fig:bm_clusters
#+ATTR_HTML: width="30px"
[[../figs/bordeaux-metropole-user-clusters-100dpi.png]]

According to the clustering done in the previous article (be careful, Qgis
shuffles the legend entries...), we can make some additional hypothesis:

+ Light-blue roads are OK, they correspond to the most trustful cluster of
  contributors (91.4%)
+ There is no group-0 road (that corresponds to cluster C2 in the previous
  article)... And that's comforting! It seems that "untrustworthy" users do not
  contribute to road or -more probably- that their contributions are quickly
  amended.
+ Other contributions are made by intermediate users: a finer analysis should
  be undertaken to decide if the corresponding elements are valid. For now, we
  can consider everything is OK, even if local patterns seem strong. Areas of
  interest should be verified (they are not necessarily of low quality!)

* Conclusion

In this second paper we detailed a whole methodology to generate
contributor-focused metadata, /i.e./ information related to each OSM user.

Then we exploited this metadata into a machine learning framework: after
reducing the dimensionality of the data through a Principle Component Analysis,
we are able to summarize the information in a small set of synthetic
components. This part of our work was also dedicated to the production of
groups of similar users, without any prior knowledge about them and their
contribution habits.

Our last target was to characterize the OSM data quality; we succeeded in it by
using the previous user clusters. We considered the last contributor of each
OSM element, and assess the quality of the latter regarding the experience of
the former.

Of course some works still have to be done, however we detailed a whole
methodology to tackle the problem. We hope you will be able to reproduce it,
and to design your own maps!

Feel free to contact us if you are interested in this topic!

* References

- Garaud, D., Delhome, R., Mercier, H. 2017. A data-oriented framework to
  assess OSM data quality (part 1): data extraction and
  description. /Geomatique Expert./ 117, July 2017.
- Websites:
  + Python Software Foundation. Python Language Reference, version 3.5. Available at http://www.python.org
  + OpenStreetMap API: Available at http://www.openstreetmap.org

